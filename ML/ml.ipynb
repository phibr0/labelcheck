{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Facharbeit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlH_pXGMKcrI"
      },
      "source": [
        "# Trainieren des Beispiel NN's aus Kapitel 2\r\n",
        "\r\n",
        "In diesem Teil wird das Beispiel NN aus Kapitel 2 meiner Facharbeit trainiert.\r\n",
        "Das MNIST Dataset enthält Bilder von Handgeschriebenen Ziffern mit einer Größe von 28x28 Pixeln.\r\n",
        "\r\n",
        "**Der Code kann direkt hier im Browser ausgeführt und auch verändert werden.**\r\n",
        "\r\n",
        "Das trainierte Modell wird anschließend als 'beispielModel_MNIST' gespeichert.\r\n",
        "\r\n",
        "## Wieso nutze ich Colab anstatt auf meinem eigenen Computer zu trainieren?\r\n",
        "\r\n",
        "Hier habe ich Zugriff auf deutlich stärkere GPUs und TPUs als die in meinem eigenen Computer was mir Zeit spart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYnJkaRag0Rd"
      },
      "source": [
        "#@title Variablen { form-width: \"30%\", display-mode: \"both\" }\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import numpy as np\n",
        "\n",
        "numOfNeuronsIn1 = 16 #@param {type:\"slider\", min:1, max:256, step:1}\n",
        "numOfNeuronsIn2 = 16 #@param {type:\"slider\", min:1, max:256, step:1}\n",
        "numOfEpochs = 10 #@param {type:\"slider\", min:1, max:256, step:1}\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() # Laden des MNIST Datasets\n",
        "# Und aufteilen in Trainigsdaten und Testdaten\n",
        "\n",
        "x_train = tf.keras.utils.normalize(x_train, axis=1) # Normalisieren des Datasets\n",
        "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
        "\n",
        "model = tf.keras.models.Sequential() ## Erstellen des Neuronalen Netzwerks\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(numOfNeuronsIn1, activation=tf.nn.sigmoid)) # Hinzufügen der Layer\n",
        "model.add(layers.Dense(numOfNeuronsIn2, activation=tf.nn.sigmoid))\n",
        "model.add(layers.Dense(10, activation=tf.nn.softmax))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']) # Kompilieren der Layer zu einem trainierfähigen Modell\n",
        "\n",
        "model.fit(x_train, y_train, epochs=numOfEpochs,validation_split=0.1) # Trainieren des Modells mit den Trainingsdaten und x Epochen\n",
        "\n",
        "model.save('beispielModel_MNIST') # Speichern des Modells"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX2gkyiINR56"
      },
      "source": [
        "# Erstellen der Trainingsdaten für Labelcheck und anschließendes trainieren\r\n",
        "\r\n",
        "Dokumentation: https://www.tensorflow.org/tutorials/load_data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks-eWumpliMf"
      },
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "import datetime\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "import tensorboard\r\n",
        "\r\n",
        "batch_size = 16\r\n",
        "img_height = 180\r\n",
        "img_width = 180\r\n",
        "data_dir='/content/drive/MyDrive/dataset'\r\n",
        "AUTOTUNE = tf.data.AUTOTUNE\r\n",
        "\r\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory( # Aufteilen der Daten in Trainingsdaten\r\n",
        "  data_dir,\r\n",
        "  validation_split=0.22,\r\n",
        "  subset='training',\r\n",
        "  seed=123,\r\n",
        "  image_size=(img_height, img_width),\r\n",
        "  batch_size=batch_size)\r\n",
        "\r\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory( # und Testdaten\r\n",
        "  data_dir,\r\n",
        "  validation_split=0.22,\r\n",
        "  subset=\"validation\",\r\n",
        "  seed=123,\r\n",
        "  image_size=(img_height, img_width),\r\n",
        "  batch_size=batch_size)\r\n",
        "\r\n",
        "class_names = train_ds.class_names\r\n",
        "print('\\nClass Names: ' + str(class_names) + '\\n') # Ausgeben der Klassen\r\n",
        "\r\n",
        "train_ds = train_ds.cache().shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE) # I/O Operationen sind in Colab sehr langsam,\r\n",
        "val_ds = val_ds.cache().shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE) # durch die Methoden cache() und prefetch() werden die Daten im RAM gehalten\r\n",
        "\r\n",
        "model = tf.keras.Sequential([\r\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),# Normalisieren der Daten, vorher waren die Daten 0-255, jetzt 0-1\r\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.1, 0.1),# Data Augmentation - Leichte zufällige Veränderungen des Bildes \"um das Dataset zu verändern\"\r\n",
        "  layers.experimental.preprocessing.RandomRotation(0.12),# Data Augmentation\r\n",
        "  layers.Conv2D(64, 5, activation='relu'),# Convoultional Layer\r\n",
        "  layers.Dropout(0.1), # Lässt Daten aus, hilft gegen Overfitting\r\n",
        "  layers.Conv2D(48, 3, activation='relu'),\r\n",
        "  layers.MaxPooling2D(),# Max Pooling Layer\r\n",
        "  layers.Conv2D(32, 3, activation='relu'),\r\n",
        "  layers.MaxPooling2D(),\r\n",
        "  layers.Conv2D(32, 3, activation='relu'),\r\n",
        "  layers.MaxPooling2D(),\r\n",
        "  layers.Flatten(),# 2D Layer --> 1D \r\n",
        "  layers.Dense(48, activation='relu'), # Normale Layer\r\n",
        "  layers.Dense(24, activation='relu'),\r\n",
        "  layers.Dense(len(class_names), activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(\r\n",
        "  optimizer='adam',# Gradient Descent mit Momentum (und mehr)\r\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "  metrics=['accuracy'])\r\n",
        "\r\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\r\n",
        "\r\n",
        "model.fit(\r\n",
        "  train_ds,\r\n",
        "  validation_data=val_ds,\r\n",
        "  epochs=160,\r\n",
        "  shuffle=True,\r\n",
        "  callbacks=[tensorboard_callback]\r\n",
        ")\r\n",
        "\r\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model) # Konvertieren des Keras Models in ein TensorFlow-Lite Model\r\n",
        "tflite_model = converter.convert()\r\n",
        "\r\n",
        "with open('model.tflite', 'wb') as f: # Speichern des Models\r\n",
        "  f.write(tflite_model)\r\n",
        "\r\n",
        "tf.keras.utils.plot_model(\r\n",
        "    model, to_file='model.png', show_shapes=True, show_dtype=False,\r\n",
        "    show_layer_names=False, rankdir='TD', expand_nested=False, dpi=96\r\n",
        ")\r\n",
        "\r\n",
        "# Achtung: Das ist kein Python Code, funktioniert aber in Colab Notebooks um das TensorBoard zu öffnen,\r\n",
        "# TensorBoard ist eine Web App um den Tainingsprozess zu analysieren\r\n",
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}